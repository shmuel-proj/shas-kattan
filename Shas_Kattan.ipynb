{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Will the Real Shas Kattan Please Stand Up\n",
    "\n",
    "There is an old tradition commonly referenced in the yeshiva community that Masekhet Ketubot is the \"Shas Kattan\" of Talmud Bavli, that is, it contains ideas that connect to just about every other area of Shas (short for \"Shisha Sidrei,\" all six orders of the Mishnah). The source and force of this idea is the subject of an article by R Tuvia Preschel, found here.\n",
    "\n",
    "Personally, I have thought that this doesn't really seem to be the case; although Ketubot does indeed have a lot of cases involving civil law and connects to many topics covered in tractates Bava Metzia, Bava Batra, and Shevu'ot, and one encounters the laws of Shabbat and Yom Tov in the first 10 pages, it doesn't contain much from Zera'im, Kodshim or Taharot (or Mo'ed really). There seem to me to be much better candidates for Shas Kattan, such as Pesachim or Nedarim, when one considers all six orders of the Mishnah.\n",
    "\n",
    "The question is, can one computationally determine which tractate is the real \"Shas Kattan\"; which tractate of the Talmud Bavli has the most varied references to the rest of Shas (all six orders)?\n",
    "\n",
    "### Approach 1: Uniqe Tractate Scoring (Using Sefaria Links Count)\n",
    "\n",
    "For each tractate of Talmud Bavli under considering, we can simply tally up how many other tractates (including Mishnah, Bavli/Yerushalmi, and Tosefta) are cited within that tractate of Talmud, with the possible highest score of 63. Perhaps it would be \"more fair\" to divide that score by the number of words in the given tractate. \n",
    "\n",
    "A slight variation on this simple counting method would be to use a points or \"scoring\" method, where additional citations from the same tractate improves the score incrementally but by decreasing amounts. For example, if the first page of a tractate quotes from Shabbat, Eruvin, and Pesachim, that's three points, and then the second page quotes Shabbat and Gittin, then it will get one more point for Gittin but only another fraction of a point for the additional Shabbat reference. My thinking is that scoring in such a matter should decrease geometrically: for the second time that tractate is referenced, give 0.5 points, then the third time, give 0.25, etc. (So, for example, if a tractate quotes Berakhot once and Shabbat thrice, it will have a score of 1 + 1.75 = 2.75).\n",
    "\n",
    "### Approach 2: Balance Between \"Six Orders\" References (Using Sefaria Links Count) \n",
    "There is another possible way of interpreting \"shas kattan\"-ness, which would refer to how well balanced all of the citations are in terms of being a more fair representation of all of the six orders of the Mishnah. A perfectly 'balanced' tractate will have 1/6 of its references to Zera'im, 1/6 to Mo'ed, etc. (although the six orders themselves are not all the same size or have the same number of tractates, so perhaps it makes sense to adjust for that). If we consider each inter-talmudic reference according to one of the six orders, which tractate is closest to the 'balanced ideal' of 1/6 of its references alloted to each order?\n",
    "\n",
    "### Approach 3: Diversity of Topics\n",
    "The Sefaria team has worked very hard on \"topic ontology,\" so that each source in their database of Tanakh and Talmud is tagged according to the topics it discusses. Although some topics are much broader than others (to the point where some topics are even included in larger ones; for example \"Egypt\" is a topic, but so is \"Egyptian slavery,\" and all the sources belonging to the latter also belong to the former) this still might be a better representation of topic diversity than how many of the 63 tractates are references and what order they are from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmO3DT2m9YoT"
   },
   "source": [
    "# Step 1: Collect, Select, Sort, and Cleanup the Sefaria Dataset\n",
    "\n",
    "First, grab Sefaria's entire \"links\" dataset and collect just the information for Shas (which requires just a teeny drop of cleaning up).\n",
    "\n",
    "\"Shas\" here is going to include all four types of mishnaic literature: Mishnah, Tosefta, Talmud Bavli, and Talmud Yerushalmi - and we don't care which one is referenced, but we don't want to count them all more than once. So I'll just collect all those references and note which \"tractate\" of the 63 tractates they belong to.\n",
    "\n",
    "Two tiny bits of cleanup are also required:\n",
    "1. Because Sefaria's dataset only includes links in one direction, we will duplicate those so that every cross-reference is bidirectional (i.e., for every time Ketubot quotes Chullin, that means Chullin also quotes Ketubot)\n",
    "1. \"Ohalot\" and \"Uktzin\" have two different spellings for their Tosefta name (go figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4KL_uvt4jUc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json, urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sjMh2UA4N0K"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/Sefaria/Sefaria-Export/master/links/links_by_book_without_commentary.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3Di1c-c4knB"
   },
   "outputs": [],
   "source": [
    "BTtractates = [ #checked that this lists accords with Sefaria title transliterations\n",
    "    \"Berakhot\", \"Shabbat\", \"Eruvin\", \"Pesachim\", \"Yoma\", \"Sukkah\", \"Beitzah\", \"Rosh Hashanah\", \"Taanit\", \"Megillah\", \"Moed Katan\", \"Chagigah\",\n",
    "    \"Yevamot\", \"Ketubot\", \"Nedarim\", \"Nazir\", \"Sotah\", \"Gittin\", \"Kiddushin\",\n",
    "    \"Bava Kamma\", \"Bava Metzia\", \"Bava Batra\", \"Sanhedrin\", \"Makkot\", \"Shevuot\", \"Avodah Zarah\", \"Horayot\",\n",
    "    \"Zevachim\", \"Menachot\", \"Chullin\", \"Bekhorot\", \"Arakhin\", \"Temurah\", \"Keritot\", \"Meilah\", \"Tamid\",\n",
    "    \"Niddah\"\n",
    "]\n",
    "\n",
    "plainshaslist = [ #this is the names of the 63 masechtos, whether mishnah or other\n",
    "    'Berakhot', \"Peah\", 'Demai', \"Kilayim\", \"Sheviit\", 'Terumot', \"Maasrot\", \"Maaser Sheni\",\n",
    "    'Challah', 'Orlah', 'Bikkurim', 'Shabbat', 'Eruvin', 'Pesachim', 'Shekalim', 'Yoma', 'Sukkah',\n",
    "    'Beitzah', 'Rosh Hashanah', 'Taanit', 'Megillah', 'Moed Katan', 'Chagigah', 'Yevamot', 'Ketubot',\n",
    "    'Nedarim', 'Nazir', 'Sotah', 'Gittin', 'Kiddushin', 'Bava Kamma', 'Bava Metzia', 'Bava Batra',\n",
    "    'Sanhedrin', 'Makkot', \"Shevuot\", 'Eduyot', 'Avodah Zarah', 'Pirkei Avot', 'Horayot', 'Zevachim',\n",
    "    'Menachot', 'Chullin', 'Bekhorot', 'Arakhin', 'Temurah', 'Keritot', \"Meilah\", 'Tamid', 'Middot',\n",
    "    'Kinnim', 'Keilim', 'Kelim', 'Ohalot', 'Oholot', \"Negaim\", 'Parah', 'Tahorot', \"Mikvaot\", 'Niddah', 'Makhshirin',\n",
    "    'Zavim', 'Tevul Yom', 'Yadayim', 'Oktzin', 'Oktsin' #yes there are two Ohalot's and two Oktzin's which we will have to merge later\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfVvPpgY5U7u"
   },
   "outputs": [],
   "source": [
    "shaslist = []\n",
    "\n",
    "#Allowing for references to Mishnah, Tosefta, and Jerusalem Talmud\n",
    "for n in plainshaslist:\n",
    "    mish = \"Mishnah \" + n\n",
    "    tos = \"Tosefta \" + n #We are actaully going to be missing Tosefta Keilim Kama, etc. but I think that's ok\n",
    "    jeru = \"Jerusalem Talmud \" + n\n",
    "    nlist = [mish, tos, jeru, n]\n",
    "    shaslist += nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "5QMpLyfI5xJr",
    "outputId": "dce84491-ffbd-407a-e5a6-6f02f9ec6253"
   },
   "outputs": [],
   "source": [
    "# Collect only cross-references of Shas, not commentaries, Tanakh, etc.\n",
    "filtered1_df = df[df['Text 1'].isin(BTtractates)]\n",
    "filtered2_df = filtered1_df[filtered1_df['Text 2'].isin(shaslist)]\n",
    "\n",
    "# Add reverse cross-references (i.e. if Berakhot quotes Shabbat 54 times, make sure that\n",
    "    # the number 54 is reflected both in 'Berakhot > Shabbat' and in 'Shabbat > Berakhot')\n",
    "# Create an empty DataFrame to collect new rows that need to be added\n",
    "new_rows = []\n",
    "\n",
    "# Iterate through the DataFrame to find rows to reverse\n",
    "for index, row in filtered2_df.iterrows():\n",
    "    if row['Text 2'] in BTtractates:\n",
    "        # Create a new row with \"Text 1\" and \"Text 2\" reversed, while keeping \"Tractate\" and \"Link Count\" the same\n",
    "        new_row = pd.DataFrame({\n",
    "            'Text 1': [row['Text 2']],\n",
    "            'Text 2': [row['Text 1']],\n",
    "            'Link Count': [row['Link Count']]\n",
    "        })\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Concatenate the new rows with the original DataFrame\n",
    "df_augmented = pd.concat([filtered2_df] + new_rows, ignore_index=True)\n",
    "\n",
    "# create a sorting scheme\n",
    "df_sort = df_augmented.copy()  # Making a copy to avoid affecting the original df\n",
    "df_sort['Text 1'] = pd.Categorical(df_sort['Text 1'], categories=BTtractates, ordered=True)\n",
    "sorted_df = df_sort.sort_values('Text 1')\n",
    "\n",
    "crossrefs = sorted_df #making another copy\n",
    "\n",
    "# Convert \"Text 2\" to a categorical type with categories in the specified order of shaslist\n",
    "crossrefs['Text 2'] = pd.Categorical(crossrefs['Text 2'], categories=shaslist, ordered=True)\n",
    "\n",
    "# Now sort the DataFrame first by \"Text 1\" and then by \"Text 2\"\n",
    "crossrefs_sorted = crossrefs.sort_values(['Text 1', 'Text 2'])\n",
    "crossrefs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7Z-RoYY6CYC"
   },
   "outputs": [],
   "source": [
    "# Now let's find the greatest number of links for each \"tractate,\" including Mishnah/Tosefta/Yerushalmi/Bavli\n",
    "\n",
    "crossrefs_copy = crossrefs_sorted #copying again just in case\n",
    "\n",
    "#We have to fix \"Oktzin\" and \"Ohalot\":\n",
    "crossrefs_copy = crossrefs_copy.replace(to_replace='Oktsin', value='Oktzin', regex=True)\n",
    "crossrefs_copy = crossrefs_copy.replace(to_replace='Oholot', value='Ohalot', regex=True)\n",
    "\n",
    "# Function to collapse all the different types of \"Tractates\" by removing prefixes\n",
    "def remove_prefixes(text):\n",
    "    # Define prefixes to remove\n",
    "    prefixes = [\"Mishnah \", \"Tosefta \", \"Jerusalem Talmud \"]\n",
    "    for prefix in prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):]  # Remove the prefix and return\n",
    "    return text  # Return the original text if no prefix matched\n",
    "\n",
    "# Create a new \"Tractate\" column that will apply the function\n",
    "crossrefs_copy['Tractate'] = crossrefs_copy['Text 2'].apply(remove_prefixes)\n",
    "\n",
    "#Re-sort\n",
    "# create a sorting scheme\n",
    "df_sort = crossrefs_copy.copy()  # Making a copy to avoid affecting the original df\n",
    "df_sort['Text 1'] = pd.Categorical(df_sort['Text 1'], categories=BTtractates, ordered=True)\n",
    "sorted_df = df_sort.sort_values('Text 1')\n",
    "sorted_df['Text 2'] = pd.Categorical(sorted_df['Text 2'], categories=shaslist, ordered=True)\n",
    "\n",
    "# Now sort the DataFrame first by \"Text 1\" and then by \"Text 2\"\n",
    "crossrefs_copy_sorted = crossrefs_copy.sort_values(['Text 1', 'Text 2'])\n",
    "\n",
    "# reset index values to go in order\n",
    "crossrefs_copy_sorted = crossrefs_copy_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossrefs_copy_sorted.to_csv('all-references.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "vuJNtfFH6LQn",
    "outputId": "267cbf2f-bc56-42d4-fbd4-be68e655a643"
   },
   "source": [
    "#If you want to view the entire dataframe, convert this block to code:\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(crossrefs_copy_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWy6yiwK-jcN"
   },
   "source": [
    "# Approach 1: Count the Number of Cross-References\n",
    "\n",
    "We can't just sum up all the references to Mishnah, Tosefta, Talmud Yerushalmi and Bavli, because many of those will be duplicates (for example, a link might exist to a Mishnah which also appears in the Tosefta and Talmuds, and give a score of 4, when really it should just be counted as 1). So we'll just count the *greatest* numerical value from among any of the possible references within a single tractate. And another small bit of cleaning up to remove self-referenes (Berakhot to Berakhot, whether Tosefta, Mishnah, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKejffQNCgdH"
   },
   "outputs": [],
   "source": [
    "# Remove all rows where 'Text 1' is 'Text 2'\n",
    "crossrefs_copy_sorted['Text 1'] = crossrefs_copy_sorted['Text 1'].astype(str) #bit of a roundabout way but otherwise python doesn't recognize the values as identical\n",
    "crossrefs_copy_sorted['Text 2'] = crossrefs_copy_sorted['Text 2'].astype(str)\n",
    "crossrefs_copy_sorted = crossrefs_copy_sorted[crossrefs_copy_sorted['Text 1'] != crossrefs_copy_sorted['Text 2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6SNpiAhB2xR"
   },
   "outputs": [],
   "source": [
    "# Create a new dataset for \"Link Count values\"\n",
    "# Initialize an empty dictionary to hold the maximum \"Link Count\" for each unique pair of \"Text 1\" and \"Tractate\"\n",
    "max_link_dict = {}\n",
    "\n",
    "# Iterate over each row in the DataFrame to populate the dictionary\n",
    "for _, row in crossrefs_copy_sorted.iterrows():\n",
    "    key = (row['Text 1'], row['Tractate'])  # Unique key is a tuple of \"Text 1\" and \"Tractate\"\n",
    "    link_count = row['Link Count']\n",
    "    text2 = row['Text 2']\n",
    "\n",
    "    # If the key is not in the dictionary or the current row's \"Link Count\" is greater than the stored value, update the dictionary\n",
    "    if key not in max_link_dict or link_count > max_link_dict[key]['Link Count']:\n",
    "        max_link_dict[key] = {'Text 2': text2, 'Link Count': link_count}\n",
    "\n",
    "# Convert the dictionary back to a DataFrame for easy viewing and manipulation\n",
    "max_link_df = pd.DataFrame([(*key, value['Text 2'], value['Link Count']) for key, value in max_link_dict.items()], columns=['Text 1', 'Tractate', 'Text 2', 'Link Count'])\n",
    "\n",
    "# Now again remove all rows where 'Text 1' is equivalent to 'Tractate'\n",
    "max_link_df['Text 1'] = max_link_df['Text 1'].astype(str)\n",
    "max_link_df['Tractate'] = max_link_df['Tractate'].astype(str)\n",
    "max_link_df = max_link_df[max_link_df['Text 1'] != max_link_df['Tractate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eandanE5CKEE"
   },
   "outputs": [],
   "source": [
    "# Create another new dataset named 'score' to count the number of unique 'Tractate' values associated with each 'Text 1' value\n",
    "\n",
    "# Group the DataFrame by 'Text 1' and then count the number of unique 'Tractate' values for each group\n",
    "score = max_link_df.groupby('Text 1')['Tractate'].nunique().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "score.columns = ['Text 1', 'Unique Tractate Count']\n",
    "\n",
    "score_sorted = score.sort_values(by='Unique Tractate Count', ascending=False)\n",
    "\n",
    "# reset index values to go in order\n",
    "score_sorted = score_sorted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRxCbv2hDhKp"
   },
   "source": [
    " In theory, I believe would be the most accurate reflection of 'shas-kattan-ness' would be to score each tractate's references as follows: give one point for every unique Tractate referenced. For the second time that tractate is referenced, give 0.5 points, then the third time, give 0.25, etc. (So, for example, if a tractate quotes Berakhot once and Shabbat thrice, it will have a score of 1 + 1.75 = 2.75). Below we score the cross-references in such a way, but perhaps a more exponential discounting would be better, I dunno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AoOaNjQUCMv3",
    "outputId": "8f30c28a-e88c-4356-b893-e11088a93932",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now for a more complicated scoring technique using a geometrically decreasing for each additional link per tractate\n",
    "data = max_link_df\n",
    "\n",
    "# Calculate the \"Link Count Score\" for each row\n",
    "data['Link Count Score'] = data['Link Count'].apply(lambda n: (1 - (0.5 ** n)) / (1 - 0.5))\n",
    "\n",
    "# Group by \"Text 1\" and sum the \"Link Count Score\" for each group\n",
    "final_scores = data.groupby('Text 1')['Link Count Score'].sum().reset_index()\n",
    "\n",
    "# Now combine them all and view the dataset\n",
    "combined_df = pd.merge(score_sorted, final_scores, left_on='Text 1', right_on='Text 1', how='left')\n",
    "\n",
    "# Also renaming columns to something understandable\n",
    "combined_df.rename(columns={'Text 1': 'Tractate', 'Unique Tractate Count': 'Unique Tractates Referenced', 'Link Count Score': 'Exponential-Decrease-Score'}, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('citation_scoring.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFSkK1JjEd3W",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (Aborted) Normalization Efforts\n",
    "Maybe it would be more fair to correct for how big the tractate is, such as by how many words it has? In the end I don't think this is actually informative, because tractates differ from each other so much more by word count than by cross-reference count. Maybe page number is more fair, but tractates also differ significantly by how many words there are on each page, so... yeah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z98FMdwYDTIM"
   },
   "outputs": [],
   "source": [
    "#Check Link Number/Score with other Talmud stats\n",
    "\n",
    "# import talmud stats from \"the internet\" as dict\n",
    "data = {\n",
    "    \"words/daf\": [1115.14, 975.11, 971.69, 933.52, 931.76, 889.70, 805.50, 815.60, 751.97, 765.57, 729.62, 731.56, 712.81, 715.04, 711.64, 691.35, 698.94, 696.08, 698.23, 687.53, 686.13, 683.51, 676.45, 681.60, 671.59, 661.24, 657.24, 646.52, 638.92, 635.14, 587.31, 582.73, 512.22, 508.82, 430.52, 384.00, 383.06],\n",
    "    \"letters/daf\": [4337.46, 3786.44, 3771.92, 3718.42, 3625.12, 3536.23, 3204.36, 3179.48, 2972.61, 2947.04, 2866.87, 2859.64, 2827.15, 2800.14, 2782.31, 2762.51, 2760.97, 2738.49, 2729.91, 2724.15, 2704.34, 2684.79, 2668.62, 2660.13, 2648.57, 2602.73, 2592.09, 2540.40, 2507.10, 2494.26, 2323.16, 2299.84, 2049.89, 1978.57, 1670.32, 1535.95, 1499.28],\n",
    "    \"words\": [70254, 26328, 12632, 28939, 104357, 26691, 22554, 39149, 24815, 17608, 113820, 59256, 18533, 84375, 83973, 51851, 23065, 50118, 84486, 23376, 82335, 60832, 40587, 32717, 73203, 73398, 57180, 67238, 24918, 89555, 32302, 69345, 4610, 89044, 27984, 8064, 34475],\n",
    "    \"letters\": [273260, 102234, 49035, 115271, 406013, 106087, 89722, 152615, 98096, 67782, 447232, 231631, 73506, 330417, 328313, 207188, 91112, 197171, 330319, 92621, 324521, 238946, 160117, 127686, 288694, 288903, 225512, 264202, 97777, 351691, 127774, 273681, 18449, 346249, 108571, 32255, 134935],\n",
    "    \"daf\": [63, 27, 13, 31, 112, 30, 28, 48, 33, 23, 156, 81, 26, 118, 118, 75, 33, 72, 121, 34, 120, 89, 60, 48, 109, 111, 87, 104, 39, 141, 55, 119, 9, 175, 65, 21, 90],\n",
    "    \"mesechta\": [\"Berakhot\", \"Keritot\", \"Horayot\", \"Megillah\", \"Sanhedrin\", \"Taanit\", \"Moed Katan\", \"Sotah\", \"Arakhin\", \"Makkot\", \"Shabbat\", \"Kiddushin\", \"Chagigah\", \"Bava Metzia\", \"Bava Kamma\", \"Avodah Zarah\", \"Temurah\", \"Niddah\", \"Yevamot\", \"Rosh Hashanah\", \"Pesachim\", \"Gittin\", \"Bekhorot\", \"Shevuot\", \"Menachot\", \"Ketubot\", \"Yoma\", \"Eruvin\", \"Beitzah\", \"Chullin\", \"Sukkah\", \"Zevachim\", \"Tamid\", \"Bava Batra\", \"Nazir\", \"Meilah\", \"Nedarim\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_mesechta = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5sxk6LPEVXT"
   },
   "outputs": [],
   "source": [
    "# Merge to include 'pages' from 'df_mesechta'\n",
    "combined_df = pd.merge(combined_df, df_mesechta[['mesechta', 'daf']], left_on='Tractate', right_on='mesechta', how='left')\n",
    "\n",
    "# Divide score by number of daf\n",
    "combined_df['Normalized to Page Number'] = combined_df['Exponential-Decrease-Score'] / combined_df['daf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Mjz1Og1HF5Gk",
    "outputId": "55a048ac-eef0-4e9d-b9a9-056702b1bff7"
   },
   "outputs": [],
   "source": [
    "# Select only the relevant columns\n",
    "final = combined_df[['Tractate', 'Unique Tractates Referenced', 'Exponential-Decrease-Score', 'Normalized to Page Number']]\n",
    "final_sorted = final.sort_values(by='Normalized to Page Number', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wDiwVBXH4mz"
   },
   "source": [
    "### Make pretty pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfkcxAolHU3D"
   },
   "outputs": [],
   "source": [
    "df = combined_df[['Tractate', 'Unique Tractates Referenced', 'Exponential-Decrease-Score']]\n",
    "df = df.sort_values(by=['Unique Tractates Referenced', 'Exponential-Decrease-Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CgAeABuIDj4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Renaming columns according to chord diagram convention\n",
    "df.columns = ['Source', 'Target', 'Value']\n",
    "\n",
    "# Define the sedarim and their tractates for color-coding the nodes\n",
    "sedarim = {\n",
    "    'Zeraim': ['Berakhot', \"Peah\", 'Demai', \"Kilayim\", \"Sheviit\", 'Terumot', \"Maasrot\", \"Maaser Sheni\", \"Challah\", 'Orlah', 'Bikkurim'],\n",
    "    'Moed': ['Shabbat', 'Eruvin', 'Pesachim', 'Shekalim', 'Yoma', 'Sukkah','Beitzah', 'Rosh Hashanah', 'Taanit', 'Megillah', 'Moed Katan', 'Chagigah'],\n",
    "    'Nashim': ['Yevamot', 'Ketubot','Nedarim', 'Nazir', 'Sotah', 'Gittin', 'Kiddushin'],\n",
    "    'Nezikin': ['Bava Kamma', 'Bava Metzia', 'Bava Batra','Sanhedrin', 'Makkot', \"Shevuot\", 'Eduyot', 'Avodah Zarah', 'Pirkei Avot', 'Horayot'],\n",
    "    'Kodshim': ['Zevachim', 'Menachot', 'Chullin', 'Bekhorot', 'Arakhin', 'Temurah', 'Keritot', \"Meilah\", 'Tamid', 'Middot','Kinnim'],\n",
    "    'Taharot': ['Keilim', 'Kelim', 'Ohalot', 'Oholot', \"Negaim\", 'Parah', 'Tahorot', \"Mikvaot\", 'Niddah', 'Makhshirin', 'Zavim', 'Tevul Yom', 'Yadayim', 'Oktzin', 'Oktsin']\n",
    "}\n",
    "\n",
    "# Map each tractate to its Seder\n",
    "tractate_to_seder = {tractate: seder for seder, tractates in sedarim.items() for tractate in tractates}\n",
    "\n",
    "# Define a color map for each of the sedarim\n",
    "seder_colors = {\n",
    "    'Zeraim': 'red',\n",
    "    'Moed': 'blue',\n",
    "    'Nashim': 'green',\n",
    "    'Nezikin': 'yellow',\n",
    "    'Kodshim': 'purple',\n",
    "    'Taharot': 'orange'\n",
    "}\n",
    "# Create a graph from the dataframe\n",
    "G = nx.from_pandas_edgelist(df, 'Source', 'Target', ['Value'])\n",
    "\n",
    "# Transfer the color list to nodes\n",
    "default_color = 'grey' # just in case there's something not in the list (i.e. misspelled, etc), I'll find it\n",
    "node_colors = [seder_colors.get(tractate_to_seder.get(node, None), default_color) for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6r1ttMgvJFcQ",
    "outputId": "1fb40d45-bdaa-47eb-c0ab-03924a720c03"
   },
   "outputs": [],
   "source": [
    "# Draw the network\n",
    "plt.figure(figsize=(18, 18))\n",
    "pos = nx.spring_layout(G, seed=42)  # for consistent layout\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2, node_color=node_colors, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "#edges = nx.draw_networkx_edges(G, pos)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPgqfVzrJQGO"
   },
   "source": [
    "# Approach 2: Balance Between \"Six Orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting again with the full dataset of references\n",
    "data = crossrefs_copy_sorted\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sedarim and their tractates for mapping\n",
    "sedarim = {\n",
    "    'Zeraim': ['Berakhot', \"Peah\", 'Demai', \"Kilayim\", \"Sheviit\", 'Terumot', \"Maasrot\", \"Maaser Sheni\", \"Challah\", 'Orlah', 'Bikkurim'],\n",
    "    'Moed': ['Shabbat', 'Eruvin', 'Pesachim', 'Shekalim', 'Yoma', 'Sukkah','Beitzah', 'Rosh Hashanah', 'Taanit', 'Megillah', 'Moed Katan', 'Chagigah'],\n",
    "    'Nashim': ['Yevamot', 'Ketubot','Nedarim', 'Nazir', 'Sotah', 'Gittin', 'Kiddushin'],\n",
    "    'Nezikin': ['Bava Kamma', 'Bava Metzia', 'Bava Batra','Sanhedrin', 'Makkot', \"Shevuot\", 'Eduyot', 'Avodah Zarah', 'Pirkei Avot', 'Horayot'],\n",
    "    'Kodshim': ['Zevachim', 'Menachot', 'Chullin', 'Bekhorot', 'Arakhin', 'Temurah', 'Keritot', \"Meilah\", 'Tamid', 'Middot','Kinnim'],\n",
    "    'Taharot': ['Keilim', 'Kelim', 'Ohalot', 'Oholot', \"Negaim\", 'Parah', 'Tahorot', \"Mikvaot\", 'Niddah', 'Makhshirin', 'Zavim', 'Tevul Yom', 'Yadayim', 'Oktzin', 'Oktsin']\n",
    "}\n",
    "\n",
    "# Map each tractate to its Seder\n",
    "tractate_to_seder = {tractate: seder for seder, tractates in sedarim.items() for tractate in tractates}\n",
    "\n",
    "# Add the 'Seder' column to the dataset\n",
    "data['Seder'] = data['Tractate'].map(tractate_to_seder)\n",
    "\n",
    "# Display the updated dataset to check the 'Seder' column\n",
    "data\n",
    "data.to_csv('citation_counts.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Summing up the link counts for each combination of tractate ('Text 1') and order ('Seder')\n",
    "grouped_data = data.groupby(['Text 1', 'Seder'])['Link Count'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculating the total number of citations for each tractate\n",
    "total_citations = grouped_data.sum(axis=1)\n",
    "grouped_data.to_csv('seder_counts.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('seder_counts.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data here\n",
    "data = pd.read_csv('seder_counts.csv')\n",
    "\n",
    "# Define category names and order\n",
    "category_names = ['Kodshim', 'Moed', 'Nashim', 'Nezikin', 'Taharot', 'Zeraim']\n",
    "new_order = ['Zeraim', 'Moed', 'Nashim', 'Nezikin', 'Kodshim', 'Taharot']\n",
    "\n",
    "## Pick a color scheme\n",
    "#colormap = 'rainbow' # Rainbow colors\n",
    "#colormap ='tab20c' # These look nicer\n",
    "#colormap = ['black' if i % 2 == 0 else 'lightgrey' for i in range(len(new_order))] #Black-and-white\n",
    "colormap = 'Greys'\n",
    "category_colors = plt.get_cmap(colormap)(np.linspace(0, 1, len(new_order))) if isinstance(colormap, str) else colormap\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "data_normalized = data[category_names].div(data[category_names].sum(axis=1), axis=0)\n",
    "\n",
    "# Reorder data\n",
    "data_normalized = data_normalized[new_order]\n",
    "\n",
    "# Plot the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 18), dpi=300)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for i, category in enumerate(new_order):\n",
    "    widths = data_normalized[category]\n",
    "    starts = data_normalized[new_order[:i]].sum(axis=1)\n",
    "    ax.barh(data['Text 1'], widths, left=starts, height=0.5, color=category_colors[i], label=category, edgecolor='black')\n",
    "\n",
    "# Adjust the legend\n",
    "ax.legend(title=\"Seder\", bbox_to_anchor=(0.5, 1.15), loc='upper center', fontsize='small', ncol=len(new_order))\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('order_balance_greyscale.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: Using Sefaria's \"Topics\" Ontology Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlaod the giant index list\n",
    "import requests\n",
    "\n",
    "url = \"https://www.sefaria.org/api/index/\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# A roundabout way of collecting all the titles of the Talmud Bavli, according to Sefaria's \n",
    "# Requires a few levels of parsing the json object \n",
    "data_list = list(data) \n",
    "fullmetadict = dict(data_list[2])\n",
    "fulldict = fullmetadict['contents']\n",
    "fulldict = fulldict[0] #this contains all of the \"Talmud\" corpus\n",
    "\n",
    "whatlist = []\n",
    "\n",
    "#now I'll collect the next level down (which includes Zera'im, Mo'ed, but also Commentaries, etc.)\n",
    "for k,v in fulldict.items():\n",
    "    if k == 'contents':\n",
    "        for i in v:\n",
    "            for new_i in v:\n",
    "                whatlist.append(new_i)\n",
    "\n",
    "bavli_titles = []\n",
    "\n",
    "# Now I can collect only the Talmud Bavli titles, which are contained in the first 6 Talmud categories\n",
    "for i in whatlist[:6]:\n",
    "    sederdict = i\n",
    "    for v in sederdict['contents']:\n",
    "        bavli_titles.append(v['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for using the Talmud Bavli name to get all the related topics\n",
    "def get_topics_for_book(book_name):\n",
    "    # Build the text reference\n",
    "    text_ref = f\"{book_name}\"\n",
    "\n",
    "    # Endpoint URL for getting topics associated with a text\n",
    "    url = f\"https://www.sefaria.org/api/related/{text_ref}\"\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, timeout = 600)\n",
    "    if response.status_code == 200: #error handling\n",
    "        topics = response.json()\n",
    "        return topics\n",
    "    else:\n",
    "        return f\"Failed to retrieve topics: Status Code {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will collect a names of all the unique topics associated with each tractate\n",
    "bavli_topics_lists = {}\n",
    "bavli_topics_num = {} #and this will keep the count\n",
    "\n",
    "for book in bavli_titles:\n",
    "    try:\n",
    "        links = get_topics_for_book(book) #See function\n",
    "    \n",
    "        topicsdetails = links['topics'] # Parse json by collecting the topics \n",
    "\n",
    "        # Make a list of each topic that is connected to this book...\n",
    "        topicnames = []\n",
    "\n",
    "        # By parsing through the dictionary object of all the topics \n",
    "        for i in topicsdetails:\n",
    "            topicnames.append(i['topic']) #and just collect the actual name of the topic\n",
    "    \n",
    "        #remove duplicates from the topic list\n",
    "        topicnames = list(set(topicnames))\n",
    "\n",
    "        # Assign the topic list, and its length, to dictionaries with the tractate name as the key\n",
    "        bavli_topics_lists[book] = topicnames \n",
    "        bavli_topics_num[book] = len(topicnames)\n",
    "        #print('successfully analyzed {book}!')\n",
    "\n",
    "    except:\n",
    "        print(f'failed to collect data for {book}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see which tractates have the most associated topics \n",
    "# Convert the dictionary into a dataframe\n",
    "topicsdf = pd.DataFrame([(key, value) for key, value in bavli_topics_num.items()], columns=['Tractate', 'Topics Count'])\n",
    "\n",
    "# create a sorting scheme\n",
    "topicsdf = topicsdf.sort_values('Topics Count', ascending=False)\n",
    "topicsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a csv\n",
    "data = (bavli_topics_lists)\n",
    "\n",
    "max_length = max(len(lst) for lst in data.values())\n",
    "\n",
    "import csv\n",
    "\n",
    "# Open a file to write to\n",
    "with open('output.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header (dictionary keys)\n",
    "    writer.writerow(data.keys())\n",
    "\n",
    "    # Write the data\n",
    "    for i in range(max_length):\n",
    "        # This line constructs a row where each entry is the ith element of the\n",
    "        # corresponding list in the dictionary; it uses an empty string '' where\n",
    "        # a list is too short to have an ith element\n",
    "        row = [data[key][i] if i < len(data[key]) else '' for key in data]\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
